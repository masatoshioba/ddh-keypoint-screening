{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcbb875-f59e-4977-90f1-49c166d6af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectron2 basic setup\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# Common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "\n",
    "# Detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0360d-a3ad-4fcb-99f1-02a2b033f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebd76c-3773-41b6-9bca-06891e050a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectron2 engine and visualization\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "# General utilities\n",
    "import numpy as np\n",
    "import os, json, cv2, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe2a6e-39bb-42e6-ab5f-9cb11193edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "# Register training dataset\n",
    "register_coco_instances(\n",
    "    \"ddh_train\",\n",
    "    {},\n",
    "    \"path/to/your/train_annotations\",  # Set the path to your COCO annotations JSON for training\n",
    "    \"path/to/your/train_images\"        # Set the path to the image root directory for the training split\n",
    ")\n",
    "\n",
    "# Register validation dataset\n",
    "register_coco_instances(\n",
    "    \"ddh_val\",\n",
    "    {},\n",
    "    \"path/to/your/val_annotations\",    # Set the path to your COCO annotations JSON for validation\n",
    "    \"path/to/your/val_images\"          # Set the path to the image root directory for the validation split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73d4b3-0243-4aed-81f6-a420ec2b635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetCatalog\n",
    "dataset_dicts = DatasetCatalog.get(\"ddh_val\")\n",
    "print(dataset_dicts[0].keys())  # 'annotations' or 'instances' があるか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c492d915-d5ab-44a2-99ca-4dbe7c2afbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# Define keypoint names, left-right pairs, and skeleton connections\n",
    "keypoint_names = ['rtacetab1', 'rtacetab2', 'rtilleum', 'rtfemur1', 'rtfemur2',\n",
    "                  'ltacetab1', 'ltacetab2', 'ltilleum', 'ltfemur1', 'ltfemur2']\n",
    "\n",
    "keypoint_flip_map = [\n",
    "    ('rtacetab1', 'ltacetab1'),\n",
    "    ('rtilleum', 'ltilleum'),\n",
    "    ('rtacetab2', 'ltacetab2'),\n",
    "    ('rtfemur1', 'ltfemur1'),\n",
    "    ('rtfemur2', 'ltfemur2')\n",
    "]\n",
    "\n",
    "keypoint_connection_rules = [\n",
    "    ('rtacetab1', 'rtacetab2', (0, 255, 255)),\n",
    "    ('rtfemur1', 'rtfemur2', (0, 100, 100)),\n",
    "    ('rtilleum', 'ltilleum', (0, 255, 0)),\n",
    "    ('ltacetab1', 'ltacetab2', (255, 255, 0)),\n",
    "    ('ltfemur1', 'ltfemur2', (255, 0, 255))\n",
    "]\n",
    "\n",
    "# Match class name to COCO JSON categories[0]['name'] (e.g., 'DDH')\n",
    "MetadataCatalog.get(\"ddh_train\").thing_classes = [\"DDH\"]\n",
    "MetadataCatalog.get(\"ddh_train\").keypoint_names = keypoint_names\n",
    "MetadataCatalog.get(\"ddh_train\").keypoint_flip_map = keypoint_flip_map\n",
    "MetadataCatalog.get(\"ddh_train\").keypoint_connection_rules = keypoint_connection_rules\n",
    "\n",
    "# Apply the same settings to 'ddh_val'\n",
    "MetadataCatalog.get(\"ddh_val\").thing_classes = [\"DDH\"]\n",
    "MetadataCatalog.get(\"ddh_val\").keypoint_names = keypoint_names\n",
    "MetadataCatalog.get(\"ddh_val\").keypoint_flip_map = keypoint_flip_map\n",
    "MetadataCatalog.get(\"ddh_val\").keypoint_connection_rules = keypoint_connection_rules\n",
    "\n",
    "# Check metadata\n",
    "metadata = MetadataCatalog.get(\"ddh_train\")\n",
    "print(metadata.as_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3557128-5e13-4832-912b-e77a01cc4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_dicts = DatasetCatalog.get(\"ddh_train\")\n",
    "val_dataset_dicts = DatasetCatalog.get(\"ddh_val\")\n",
    "\n",
    "# Quick sanity check of counts (example)\n",
    "print(f\"Train images: {len(train_dataset_dicts)}\")\n",
    "print(f\"Validation images: {len(val_dataset_dicts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f75d44-9ef9-4f7e-bf9e-8850b1da7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = MetadataCatalog.get(\"ddh_train\")\n",
    "metadata.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c8ceae-1494-4980-8131-0e61a171d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "metadata = MetadataCatalog.get(\"ddh_train\")\n",
    "dataset_dicts = DatasetCatalog.get(\"ddh_train\")\n",
    "def cv2_imshow(im):\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(), plt.imshow(im), plt.axis('off');\n",
    "\n",
    "for d in random.sample(dataset_dicts, 5):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)   \n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    cv2_imshow(vis.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45364e-7347-4904-9b49-66a0dcdb71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "import os\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.MODEL.DEVICE = \"cuda\"\n",
    "\n",
    "# Model to use (ResNet-50 + FPN, pretrained on COCO keypoints)\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "# Dataset names (as registered by register_coco_instances)\n",
    "cfg.DATASETS.TRAIN = (\"ddh_train\",)\n",
    "cfg.DATASETS.TEST = (\"ddh_val\",)\n",
    "\n",
    "# Output directory (for checkpoints and logs)\n",
    "cfg.OUTPUT_DIR = \"path/to/your/output_dir\"  # Set the directory to save checkpoints and logs\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Evaluation period (in iterations)\n",
    "cfg.TEST.EVAL_PERIOD = 500  # e.g., evaluate on the validation set every 500 iterations\n",
    "\n",
    "# Other training settings (batch size, etc.)\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.001\n",
    "cfg.SOLVER.MAX_ITER = 35000\n",
    "cfg.SOLVER.STEPS = []  # no step LR decay\n",
    "\n",
    "# Number of keypoints (10)\n",
    "cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 10\n",
    "cfg.TEST.KEYPOINT_OKS_SIGMAS = [0.05] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbabd37-77a9-4b1b-9bd4-985a25c998e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365508ed-2273-41d8-83cf-1d0ea389ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine.hooks import HookBase\n",
    "import os\n",
    "\n",
    "class BestCheckpointer(HookBase):\n",
    "    def __init__(self, eval_period, checkpointer, val_metric_name=\"val_loss\", mode=\"min\", file_prefix=\"model_best\"):\n",
    "        self._period = eval_period\n",
    "        self._checkpointer = checkpointer\n",
    "        self._val_metric = val_metric_name\n",
    "        self._mode = mode\n",
    "        self._file_prefix = file_prefix\n",
    "        self._best_metric = None\n",
    "\n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        if next_iter % self._period != 0 and next_iter != self.trainer.max_iter:\n",
    "            return\n",
    "\n",
    "        latest_metric = self.trainer.storage.history(self._val_metric).latest()\n",
    "        if latest_metric is None:\n",
    "            return\n",
    "\n",
    "        is_better = (\n",
    "            self._best_metric is None or\n",
    "            (self._mode == \"min\" and latest_metric < self._best_metric) or\n",
    "            (self._mode == \"max\" and latest_metric > self._best_metric)\n",
    "        )\n",
    "\n",
    "        if is_better:\n",
    "            self._best_metric = latest_metric\n",
    "            save_name = f\"{self._file_prefix}\"\n",
    "            self._checkpointer.save(save_name)\n",
    "            print(f\"[BestCheckpointer] Saved new best checkpoint to '{save_name}.pth' \"\n",
    "                  f\"(val_loss: {latest_metric:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66027e85-aa28-4b66-84c3-0b09bb3a7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from detectron2.engine.hooks import HookBase\n",
    "\n",
    "class LossEvalHook(HookBase):\n",
    "    def __init__(self, eval_period, model, data_loader):\n",
    "        self._model = model\n",
    "        self._data_loader = data_loader\n",
    "        self._eval_period = eval_period\n",
    "\n",
    "    def _do_loss_eval(self):\n",
    "        total = 0.0\n",
    "        count = 0\n",
    "        num_devices = torch.distributed.get_world_size() if torch.distributed.is_initialized() else 1\n",
    "\n",
    "        # self._model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, inputs in enumerate(self._data_loader):\n",
    "                if \"instances\" not in inputs[0]:  # ← inputs[0] に 'instances' がないと loss が出せない\n",
    "                    print(f\"[LossEvalHook] Warning: 'instances' not in inputs[{idx}]\")\n",
    "                    continue\n",
    "                try:\n",
    "                    loss_dict = self._model(inputs)\n",
    "                    print(f\"[LossEvalHook] loss_dict at idx={idx}:\", loss_dict)\n",
    "                    losses = sum(loss_dict.values())\n",
    "                    total += losses.item()\n",
    "                    count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"[LossEvalHook] Error at idx={idx}: {e}\")\n",
    "        self._model.train()\n",
    "        mean_loss = total / count if count > 0 else float(\"nan\")\n",
    "        print(f\"[LossEvalHook] Validation Loss: {mean_loss:.4f}, Count: {count}\")\n",
    "        self.trainer.storage.put_scalar(\"val_loss\", mean_loss)\n",
    "        return mean_loss\n",
    "\n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        if next_iter % self._eval_period == 0 and next_iter != self.trainer.max_iter:\n",
    "            self._do_loss_eval()\n",
    "\n",
    "    def after_train(self):\n",
    "        self._do_loss_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6cfe9-3c41-469d-aeb8-8e2d78e1fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.events import EventStorage\n",
    "from detectron2.data import build_detection_test_loader, DatasetMapper\n",
    "\n",
    "# Set is_train=True in the mapper so that 'instances' are included\n",
    "val_loader = build_detection_test_loader(\n",
    "    cfg,\n",
    "    cfg.DATASETS.TEST[0],\n",
    "    mapper=DatasetMapper(cfg, is_train=True)\n",
    ")\n",
    "\n",
    "trainer = MyTrainer(cfg)\n",
    "\n",
    "# Register custom hooks\n",
    "trainer.register_hooks([\n",
    "    LossEvalHook(cfg.TEST.EVAL_PERIOD, trainer.model, val_loader),\n",
    "    BestCheckpointer(cfg.TEST.EVAL_PERIOD, trainer.checkpointer, \"val_loss\", mode=\"min\", file_prefix=\"best_model\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9e105-584c-4c2b-b8de-c785bcffcede",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.register_hooks([\n",
    "    LossEvalHook(cfg.TEST.EVAL_PERIOD, trainer.model, val_loader),\n",
    "    BestCheckpointer(cfg.TEST.EVAL_PERIOD, trainer.checkpointer, \"val_loss\", mode=\"min\", file_prefix=\"best_model0804\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6dc82-5777-4883-b459-1ef012e1eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.config import get_cfg\n",
    "cfg = get_cfg()\n",
    "print(cfg.INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ccbb3-6968-4472-95a4-3ba362299994",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49009f1c-bb83-4a13-b7e7-78313ac678b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to markdown --stdout keypoint11.ipynb > notebook_log0805.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11235e-9933-44d1-aa8c-86c2c3b4767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r runs runs_backup_$(date +%Y%m%d_%H%M%S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e87fe9-153f-4f58-89bf-fc9c52df29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir ./output_ddh0804/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b057e30-a2a5-4f72-81a8-b2c681b8b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ./output_ddh0804/ -name \"events.out.tfevents.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c773b2-5fe3-4978-8d06-775adc20ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir .output_ddh0804/your_experiment_name/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa949ae1-5009-490c-83af-ef2592d677ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.config.config import CfgNode as CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61acc4e-ae49-4ac6-a19a-7de75daf2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.DATALOADER.NUM_WORKERS = 0\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.001  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 20000  # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   #128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 1000\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # DDHxp1\n",
    "cfg.MODEL.RETINANET.NUM_CLASSES = 1\n",
    "cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 10\n",
    "cfg.TEST.KEYPOINT_OKS_SIGMAS = np.ones((10, 1), dtype=float).tolist()\n",
    "cfg.TEST.EVAL_PERIOD = 1000\n",
    "from datetime import datetime\n",
    "\n",
    "# Make the output directory unique by timestamp\n",
    "cfg.OUTPUT_DIR = f\"./output_ddh/run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "trainer = MyTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bbf108-62d7-42f5-88e0-eb589042ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "experiment_folder = 'output'\n",
    "\n",
    "def load_json_arr(json_path):\n",
    "    lines = []\n",
    "    with open(json_path, 'r') as f:\n",
    "        for line in f:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "experiment_metrics = load_json_arr(\"path/to/your/metrics_json\")  # Set the path to Detectron2 training metrics JSON (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f89766-ae0c-4358-a2fe-31e23d8e4775",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 8]\n",
    "plt.plot(\n",
    "    [x['iteration'] for x in experiment_metrics if 'validation_loss' in x], \n",
    "    [x['validation_loss'] for x in experiment_metrics if 'validation_loss' in x], color='blue')\n",
    "plt.plot(\n",
    "    [x['iteration'] for x in experiment_metrics if 'validation_loss' in x], \n",
    "    [x['loss_keypoint'] for x in experiment_metrics if 'validation_loss' in x], color='orange')\n",
    "\n",
    "plt.legend(['validation_loss', 'loss_keypoint'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad7967c-1d77-405d-941b-6f35e44fbd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment_metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523fd849-4b66-45c0-a51c-3c98b5e75e39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (detectron2)",
   "language": "python",
   "name": "detectron2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
